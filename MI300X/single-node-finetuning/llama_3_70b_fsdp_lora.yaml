# script parameters
model_id: "/data/models/Meta-Llama-3-70B" # Hugging Face model id,
dataset_path: "."                      # path to dataset,
max_seq_length:  2048 # 2048              # max sequence length for model and packing of the dataset,
# training parameters,
output_dir: "./llama-3-70b-hf-no-robot" # Temporary output directory for model checkpoints,
report_to: "mlflow"               # report metrics to tensorboard,
learning_rate: 0.00001                  # learning rate 2e-4,
lr_scheduler_type: "constant"          # learning rate scheduler,
num_train_epochs: 1                    # number of training epochs,
per_device_train_batch_size: 2         # batch size per device during training,
per_device_eval_batch_size: 1          # batch size for evaluation,
gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass,
optim: adamw_torch                     # use torch adamw optimizer,
logging_steps: 1                      # log every 10 steps,
save_strategy: epoch                   # save checkpoint every epoch,
evaluation_strategy: epoch             # evaluate every epoch,
max_grad_norm: 1                     # max gradient norm,
warmup_ratio: 0.0                     # warmup ratio,
bf16: true                             # use bfloat16 precision,
tf32: false                             # use tf32 precision,
gradient_checkpointing: false           # use gradient checkpointing to save memory,
# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp,
fsdp: "full_shard auto_wrap" # remove offload if enough GPU memory,
fsdp_config:
  backward_prefetch: "backward_pre"
  forward_prefetch: "false"
  use_orig_params: "false"